# Medical Assistant RAG System

A complete Retrieval-Augmented Generation (RAG) system that answers medical questions based on document corpus using [LangChain](https://python.langchain.com/docs/introduction), [FAISS](https://github.com/facebookresearch/faiss), and [Cohere](https://docs.cohere.com/cohere-documentation).

## ğŸš€ Live Demo

**Deployed URL**:
- [Streamlit Cloud](https://medicalassistant-luuthanhtung.streamlit.app/)
- [HuggingFace Space](https://huggingface.co/spaces/tuniel/medicalassistant-tungluu)

## Architecture
![Architecture](assets/architecture.jpg)
### Core Components
- **DocumentProcessor**: Handles file ingestion and text chunking
- **VectorStore**: Manages FAISS index and similarity search
- **RAGPipeline**: Orchestrates retrieval and generation
- **Logger**: Provides structured logging throughout the system

## ğŸ“‹ Features

- **Document Ingestion**: Support for PDF and TXT files
- **Chunking**: text splitting with overlap
- **Vector Search**: FAISS-powered semantic retrieval
- **LLM Generation**: [Cohere LLM](https://docs.cohere.com/v2/docs/models) for accurate responses
- **Web Interface**: Clean Streamlit UI
- **Comprehensive Logging**: Query tracking and debug information
- **Source Attribution**: Shows retrieved document snippets

## ğŸ› ï¸ Tech Stack

- **Backend**: Python, LangChain, FAISS, Cohere
- **LLM**: Cohere command-r-plus
- **Frontend**: Streamlit
- **Embeddings**: Cohere embed-english-v3.0
- **Deployment**: Streamlit Cloud / HuggingFace Spaces

## ğŸ“ Project Structure

```
medical-rag-assistant/
medical_assistant/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ requirements.txt                # Dependencies (updated versions)
â”œâ”€â”€ .env                            # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                # Configuration management
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ components/               # RAG Components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py       # Document loading component
â”‚   â”‚   â”œâ”€â”€ data_processor.py    # Text processing & chunking
â”‚   â”‚   â”œâ”€â”€ embedding.py         # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # Vector storage management
â”‚   â”‚   â”œâ”€â”€ retriever.py         # Information retrieval
â”‚   â”‚   â””â”€â”€ generator.py         # Response generation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py      # Orchestrates retrieval and generation
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py            # Logging utilities
â”‚       â””â”€â”€ helpers.py           # Helper functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/               # Source documents
â”‚   â””â”€â”€ processed/              # Processed data cache
â”œâ”€â”€ tests/                      # Unit test
â”‚    â”œâ”€â”€ __init__.py
â”‚    â”œâ”€â”€ test_components/
â”‚    â”‚   â”œâ”€â”€ test_data_loader.py
â”‚    â”‚   â”œâ”€â”€ test_data_processor.py
â”‚    â”‚   â”œâ”€â”€ test_embedding.py
â”‚    â”‚   â”œâ”€â”€ test_vector_store.py
â”‚    â”‚   â”œâ”€â”€ test_retriever.py
â”‚    â”‚   â””â”€â”€ test_generator.py
â”‚    â””â”€â”€ test_integration/
â”‚        â””â”€â”€ test_rag_pipeline.py
â””â”€â”€ logs/                       # Application logs
```

## ğŸš€ Quick Start

### ğŸ“‹ Requirements

- Python 3.10
- [Cohere API key](https://docs.cohere.com/v2/docs/rate-limits) - [How to get it?](https://docs.aicontentlabs.com/articles/cohere-api-key/)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/medical-rag-assistant.git
cd medical-rag-assistant
```

### 2.1. Run the Application on local

```bash
pip install -r requirements.txt
streamlit run app.py
```
### 2.2. OR Run the Application with docker

```bash
docker build -t streamlit-app .
docker run -p 8501:8501 streamlit-app
```

The app will be available at `http://localhost:8501`

## ğŸ”§ Configuration
You can modify this in [`medical_assistant\config\settings.py`](config/settings.py)
### Document Processing
- **Chunk Size**: 1000 characters
- **Chunk Overlap**: 200 characters  
- **Retrieval**: Top 3 most relevant chunks

### LLM Settings
- **Model**: command-r-plus
- **Temperature**: 0.1 (for consistent medical responses)
- **Max Tokens**: 1000

## ğŸ“Š Logging

The system logs:
- Console logging is always enabled for real-time debugging.
- File logging can be enabled or disabled using the `enable_file_logging` flag in [settings](config/settings.py).
- User queries and timestamps
- Retrieved document chunks with scores
- LLM responses and processing times
- Error handling and debug information

Logs are stored in `logs/rag_system.log` with rotation.
### Logging Monitor 

Run ELK stack with `Filebeat`
```bash
cd /src/elk
docker compose -f elk-docker-compose.yml -f extensions/filebeat/filebeat-compose.yml up -d
```
Quickly run a container so that `Filebeat` can collect logs from it
```bash
docker run -p 8501:8501 streamlit-app
```
#### Access services
- Kibana: http://localhost:5601 with `username/password` is `elastic/changeme`

![kibana interface](assets/logs_monitor.png)

## ğŸ§ª Sample Documents

The `data/sample_documents/` folder contains example medical documents:
- Subdocs Alzheimer's disease
- Subdocs beriberi desease

## ğŸ¬ Video Demo
[![Video Demo App](https://img.youtube.com/vi/ugD-2esz9-k/default.jpg)](https://www.youtube.com/watch?v=ugD-2esz9-k)


