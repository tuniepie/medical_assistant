# Medical Assistant RAG System

A complete Retrieval-Augmented Generation (RAG) system that answers medical questions based on document corpus using [LangChain](https://python.langchain.com/docs/introduction), [FAISS](https://github.com/facebookresearch/faiss), and [Cohere](https://docs.cohere.com/cohere-documentation).

## ğŸš€ Live Demo

**Deployed URL**: [Your deployment URL will go here]

## ğŸ“‹ Features

- **Document Ingestion**: Support for PDF and TXT files
- **Chunking**: text splitting with overlap
- **Vector Search**: FAISS-powered semantic retrieval
- **LLM Generation**: [Cohere LLM](https://docs.cohere.com/v2/docs/models) for accurate responses
- **Web Interface**: Clean Streamlit UI
- **Comprehensive Logging**: Query tracking and debug information
- **Source Attribution**: Shows retrieved document snippets

## ğŸ› ï¸ Tech Stack

- **Backend**: Python, LangChain, FAISS, Cohere
- **LLM**: Cohere command-r-plus
- **Frontend**: Streamlit
- **Embeddings**: Cohere embed-english-v3.0
- **Deployment**: Streamlit Cloud / HuggingFace Spaces

## ğŸ“ Project Structure

```
medical-rag-assistant/
medical_assistant/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ requirements.txt                # Dependencies (updated versions)
â”œâ”€â”€ .env.example                   # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                # Configuration management
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ components/               # RAG Components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py       # Document loading component
â”‚   â”‚   â”œâ”€â”€ data_processor.py    # Text processing & chunking
â”‚   â”‚   â”œâ”€â”€ embedding.py         # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # Vector storage management
â”‚   â”‚   â”œâ”€â”€ retriever.py         # Information retrieval
â”‚   â”‚   â””â”€â”€ generator.py         # Response generation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py      # Orchestrates all components
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py            # Logging utilities
â”‚       â””â”€â”€ helpers.py           # Helper functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/               # Source documents
â”‚   â””â”€â”€ processed/              # Processed data cache
â”œâ”€â”€ tests/                      # Test file
â”‚    â”œâ”€â”€ __init__.py
â”‚    â”œâ”€â”€ test_components/
â”‚    â”‚   â”œâ”€â”€ test_data_loader.py
â”‚    â”‚   â”œâ”€â”€ test_data_processor.py
â”‚    â”‚   â”œâ”€â”€ test_embedding.py
â”‚    â”‚   â”œâ”€â”€ test_vector_store.py
â”‚    â”‚   â”œâ”€â”€ test_retriever.py
â”‚    â”‚   â””â”€â”€ test_generator.py
â”‚    â””â”€â”€ test_integration/
â”‚        â””â”€â”€ test_rag_pipeline.py
â””â”€â”€ logs/                       # Application logs
```

## ğŸš€ Quick Start

## ğŸ“‹ Requirements

- Python 3.10
- Cohere API key

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/medical-rag-assistant.git
cd medical-rag-assistant
```

### 2. Set Up Environment Variables

```bash
cp .env.example .env
```

Edit `.env` and add your Cohere API key:
```
COHERE_API_KEY=your_cohere_api_key_here
```

### 3. Run the Application on local

```bash
pip install -r requirements.txt
streamlit run app.py
```
### 4. Run the Application on local
```bash
docker build -t streamlit-app .
docker run -p 8501:8501 streamlit-app
```

The app will be available at `http://localhost:8501`

## ğŸ“š Usage

1. **Upload Documents**: Use the sidebar to upload PDF or TXT files
2. **Ask Questions**: Enter your medical question in the text input
3. **View Results**: See the AI response along with source documents
4. **Check Logs**: Monitor the logs section for debugging information

## ğŸ”§ Configuration

### Document Processing
- **Chunk Size**: 1000 characters
- **Chunk Overlap**: 200 characters  
- **Retrieval**: Top 3 most relevant chunks

### LLM Settings
- **Model**: command-r-plus
- **Temperature**: 0.1 (for consistent medical responses)
- **Max Tokens**: 1000

## ğŸ“Š Logging

The system logs:
- User queries and timestamps
- Retrieved document chunks with scores
- LLM responses and processing times
- Error handling and debug information

Logs are stored in `logs/rag_system.log` with rotation.

## ğŸš€ Deployment

### Streamlit Cloud
1. Push code to GitHub
2. Connect repository to Streamlit Cloud
3. Add environment variables in Streamlit settings
4. Deploy automatically

### HuggingFace Spaces
1. Create new Space with Streamlit SDK
2. Upload repository files
3. Add OpenAI API key to Space secrets
4. Space will auto-deploy

## ğŸ§ª Sample Documents

The `data/sample_documents/` folder contains example medical documents:
- Medical clinic policies
- Treatment guidelines
- General health information

## ğŸ” API Structure

### Core Components

- **DocumentProcessor**: Handles file ingestion and text chunking
- **VectorStore**: Manages FAISS index and similarity search
- **RAGPipeline**: Orchestrates retrieval and generation
- **Logger**: Provides structured logging throughout the system




## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---